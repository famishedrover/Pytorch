{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# MNIST image size is 28x28x1 = 784\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "train_dataset = datasets.MNIST(root='./data',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "test_dataset = datasets.MNIST(root='./data',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DatasetLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression Class\n",
    "class LogisticRegression (nn.Module) :\n",
    "    def __init__(self,input_size,num_classes) :\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,num_classes)\n",
    "    def forward(self,x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = LogisticRegression(input_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function of Logistic Regression\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer \n",
    "optimizer = torch.optim.SGD( model.parameters() , lr=learning_rate ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [100/600] , Loss: 2.1955\n",
      "Epoch: [1/10], Step: [200/600] , Loss: 2.0834\n",
      "Epoch: [1/10], Step: [300/600] , Loss: 2.0026\n",
      "Epoch: [1/10], Step: [400/600] , Loss: 1.9274\n",
      "Epoch: [1/10], Step: [500/600] , Loss: 1.8540\n",
      "Epoch: [1/10], Step: [600/600] , Loss: 1.7819\n",
      "Epoch: [2/10], Step: [100/600] , Loss: 1.6882\n",
      "Epoch: [2/10], Step: [200/600] , Loss: 1.6093\n",
      "Epoch: [2/10], Step: [300/600] , Loss: 1.6131\n",
      "Epoch: [2/10], Step: [400/600] , Loss: 1.6268\n",
      "Epoch: [2/10], Step: [500/600] , Loss: 1.4644\n",
      "Epoch: [2/10], Step: [600/600] , Loss: 1.3500\n",
      "Epoch: [3/10], Step: [100/600] , Loss: 1.4456\n",
      "Epoch: [3/10], Step: [200/600] , Loss: 1.4030\n",
      "Epoch: [3/10], Step: [300/600] , Loss: 1.2652\n",
      "Epoch: [3/10], Step: [400/600] , Loss: 1.3120\n",
      "Epoch: [3/10], Step: [500/600] , Loss: 1.2256\n",
      "Epoch: [3/10], Step: [600/600] , Loss: 1.2222\n",
      "Epoch: [4/10], Step: [100/600] , Loss: 1.2062\n",
      "Epoch: [4/10], Step: [200/600] , Loss: 1.2714\n",
      "Epoch: [4/10], Step: [300/600] , Loss: 1.0523\n",
      "Epoch: [4/10], Step: [400/600] , Loss: 1.1316\n",
      "Epoch: [4/10], Step: [500/600] , Loss: 1.1811\n",
      "Epoch: [4/10], Step: [600/600] , Loss: 1.0995\n",
      "Epoch: [5/10], Step: [100/600] , Loss: 1.0550\n",
      "Epoch: [5/10], Step: [200/600] , Loss: 1.0804\n",
      "Epoch: [5/10], Step: [300/600] , Loss: 1.0755\n",
      "Epoch: [5/10], Step: [400/600] , Loss: 1.0493\n",
      "Epoch: [5/10], Step: [500/600] , Loss: 0.9897\n",
      "Epoch: [5/10], Step: [600/600] , Loss: 1.0406\n",
      "Epoch: [6/10], Step: [100/600] , Loss: 0.9701\n",
      "Epoch: [6/10], Step: [200/600] , Loss: 0.9626\n",
      "Epoch: [6/10], Step: [300/600] , Loss: 1.0392\n",
      "Epoch: [6/10], Step: [400/600] , Loss: 0.9423\n",
      "Epoch: [6/10], Step: [500/600] , Loss: 0.9143\n",
      "Epoch: [6/10], Step: [600/600] , Loss: 0.9417\n",
      "Epoch: [7/10], Step: [100/600] , Loss: 0.8937\n",
      "Epoch: [7/10], Step: [200/600] , Loss: 0.8853\n",
      "Epoch: [7/10], Step: [300/600] , Loss: 0.8356\n",
      "Epoch: [7/10], Step: [400/600] , Loss: 0.8479\n",
      "Epoch: [7/10], Step: [500/600] , Loss: 0.8945\n",
      "Epoch: [7/10], Step: [600/600] , Loss: 0.8898\n",
      "Epoch: [8/10], Step: [100/600] , Loss: 0.7790\n",
      "Epoch: [8/10], Step: [200/600] , Loss: 0.8402\n",
      "Epoch: [8/10], Step: [300/600] , Loss: 0.7915\n",
      "Epoch: [8/10], Step: [400/600] , Loss: 0.7779\n",
      "Epoch: [8/10], Step: [500/600] , Loss: 0.8048\n",
      "Epoch: [8/10], Step: [600/600] , Loss: 0.8318\n",
      "Epoch: [9/10], Step: [100/600] , Loss: 0.8711\n",
      "Epoch: [9/10], Step: [200/600] , Loss: 0.7409\n",
      "Epoch: [9/10], Step: [300/600] , Loss: 0.7725\n",
      "Epoch: [9/10], Step: [400/600] , Loss: 0.7131\n",
      "Epoch: [9/10], Step: [500/600] , Loss: 0.8070\n",
      "Epoch: [9/10], Step: [600/600] , Loss: 0.6910\n",
      "Epoch: [10/10], Step: [100/600] , Loss: 0.7040\n",
      "Epoch: [10/10], Step: [200/600] , Loss: 0.7377\n",
      "Epoch: [10/10], Step: [300/600] , Loss: 0.7129\n",
      "Epoch: [10/10], Step: [400/600] , Loss: 0.6776\n",
      "Epoch: [10/10], Step: [500/600] , Loss: 0.7516\n",
      "Epoch: [10/10], Step: [600/600] , Loss: 0.8565\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "for epoch in range(epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader) :\n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        #Forward Prop -> loss -> Backward Prop -> Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        #Logs\n",
    "        if (i+1)%100 == 0 :\n",
    "            print 'Epoch: [%d/%d], Step: [%d/%d] , Loss: %.4f'%(epoch+1,\n",
    "                                                               epochs,\n",
    "                                                               i+1,\n",
    "                                                               len(train_dataset)//batch_size,\n",
    "                                                               loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on  10000  test images is: 85.51 %\n"
     ]
    }
   ],
   "source": [
    "# Testing Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images , labels in test_loader :\n",
    "    images = Variable(images.view(-1,28*28))\n",
    "    # do not make lables as Variable here.\n",
    "    outputs = model(images)\n",
    "\n",
    "    _,predicted = torch.max(outputs.data,1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "print 'Accuracy on ',total,' test images is:',(correct/float(total))*100 , '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights.\n",
    "# torch.save(model.state_dict(),'logisticRmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
