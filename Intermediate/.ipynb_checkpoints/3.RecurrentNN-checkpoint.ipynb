{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParams\n",
    "seq_length    = 28\n",
    "input_size    = 28\n",
    "hidden_size   = 128\n",
    "num_layers    = 2\n",
    "num_classes   = 10\n",
    "batch_size    = 100\n",
    "num_epochs    = 2 \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST \n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = nn.LSTM(10, 20, 2)\n",
    "# input = Variable(torch.randn(5, 3, 10))\n",
    "# # print input.size(0)\n",
    "# h0 = Variable(torch.randn(2, 3, 20))\n",
    "# c0 = Variable(torch.randn(2, 3, 20))\n",
    "# output, hn = rnn(input, (h0, c0))\n",
    "# print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers,num_classes):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h0 = Variable(torch.zeros(self.num_layers,x.size(0),self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.num_layers,x.size(0),self.hidden_size))\n",
    "        \n",
    "        out,_ = self.lstm(x,(h0,c0))\n",
    "        \n",
    "        #hidden state of last time_step\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "rnn = RNN(input_size,hidden_size,num_layers,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & Optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [10/600], Loss: 0.1480\n",
      "Epoch [1/2], Step [20/600], Loss: 0.1906\n",
      "Epoch [1/2], Step [30/600], Loss: 0.0748\n",
      "Epoch [1/2], Step [40/600], Loss: 0.1605\n",
      "Epoch [1/2], Step [50/600], Loss: 0.3945\n",
      "Epoch [1/2], Step [60/600], Loss: 0.2286\n",
      "Epoch [1/2], Step [70/600], Loss: 0.1657\n",
      "Epoch [1/2], Step [80/600], Loss: 0.1875\n",
      "Epoch [1/2], Step [90/600], Loss: 0.1135\n",
      "Epoch [1/2], Step [100/600], Loss: 0.0716\n",
      "Epoch [1/2], Step [110/600], Loss: 0.1758\n",
      "Epoch [1/2], Step [120/600], Loss: 0.2707\n",
      "Epoch [1/2], Step [130/600], Loss: 0.1936\n",
      "Epoch [1/2], Step [140/600], Loss: 0.0796\n",
      "Epoch [1/2], Step [150/600], Loss: 0.0674\n",
      "Epoch [1/2], Step [160/600], Loss: 0.1413\n",
      "Epoch [1/2], Step [170/600], Loss: 0.1469\n",
      "Epoch [1/2], Step [180/600], Loss: 0.1939\n",
      "Epoch [1/2], Step [190/600], Loss: 0.0997\n",
      "Epoch [1/2], Step [200/600], Loss: 0.1290\n",
      "Epoch [1/2], Step [210/600], Loss: 0.2057\n",
      "Epoch [1/2], Step [220/600], Loss: 0.1666\n",
      "Epoch [1/2], Step [230/600], Loss: 0.0410\n",
      "Epoch [1/2], Step [240/600], Loss: 0.0919\n",
      "Epoch [1/2], Step [250/600], Loss: 0.1558\n",
      "Epoch [1/2], Step [260/600], Loss: 0.1454\n",
      "Epoch [1/2], Step [270/600], Loss: 0.0553\n",
      "Epoch [1/2], Step [280/600], Loss: 0.0646\n",
      "Epoch [1/2], Step [290/600], Loss: 0.0576\n",
      "Epoch [1/2], Step [300/600], Loss: 0.1002\n",
      "Epoch [1/2], Step [310/600], Loss: 0.1149\n",
      "Epoch [1/2], Step [320/600], Loss: 0.2044\n",
      "Epoch [1/2], Step [330/600], Loss: 0.1291\n",
      "Epoch [1/2], Step [340/600], Loss: 0.0802\n",
      "Epoch [1/2], Step [350/600], Loss: 0.0634\n",
      "Epoch [1/2], Step [360/600], Loss: 0.0620\n",
      "Epoch [1/2], Step [370/600], Loss: 0.0230\n",
      "Epoch [1/2], Step [380/600], Loss: 0.0708\n",
      "Epoch [1/2], Step [390/600], Loss: 0.2677\n",
      "Epoch [1/2], Step [400/600], Loss: 0.0993\n",
      "Epoch [1/2], Step [410/600], Loss: 0.1610\n",
      "Epoch [1/2], Step [420/600], Loss: 0.1578\n",
      "Epoch [1/2], Step [430/600], Loss: 0.0511\n",
      "Epoch [1/2], Step [440/600], Loss: 0.1800\n",
      "Epoch [1/2], Step [450/600], Loss: 0.1182\n",
      "Epoch [1/2], Step [460/600], Loss: 0.0395\n",
      "Epoch [1/2], Step [470/600], Loss: 0.1369\n",
      "Epoch [1/2], Step [480/600], Loss: 0.1138\n",
      "Epoch [1/2], Step [490/600], Loss: 0.1345\n",
      "Epoch [1/2], Step [500/600], Loss: 0.0695\n",
      "Epoch [1/2], Step [510/600], Loss: 0.0622\n",
      "Epoch [1/2], Step [520/600], Loss: 0.1049\n",
      "Epoch [1/2], Step [530/600], Loss: 0.0847\n",
      "Epoch [1/2], Step [540/600], Loss: 0.0473\n",
      "Epoch [1/2], Step [550/600], Loss: 0.0845\n",
      "Epoch [1/2], Step [560/600], Loss: 0.0481\n",
      "Epoch [1/2], Step [570/600], Loss: 0.0531\n",
      "Epoch [1/2], Step [580/600], Loss: 0.0619\n",
      "Epoch [1/2], Step [590/600], Loss: 0.0753\n",
      "Epoch [1/2], Step [600/600], Loss: 0.0829\n",
      "Epoch [2/2], Step [10/600], Loss: 0.0681\n",
      "Epoch [2/2], Step [20/600], Loss: 0.0797\n",
      "Epoch [2/2], Step [30/600], Loss: 0.0344\n",
      "Epoch [2/2], Step [40/600], Loss: 0.0971\n",
      "Epoch [2/2], Step [50/600], Loss: 0.1204\n",
      "Epoch [2/2], Step [60/600], Loss: 0.0812\n",
      "Epoch [2/2], Step [70/600], Loss: 0.1333\n",
      "Epoch [2/2], Step [80/600], Loss: 0.0597\n",
      "Epoch [2/2], Step [90/600], Loss: 0.1716\n",
      "Epoch [2/2], Step [100/600], Loss: 0.0998\n",
      "Epoch [2/2], Step [110/600], Loss: 0.0836\n",
      "Epoch [2/2], Step [120/600], Loss: 0.1656\n",
      "Epoch [2/2], Step [130/600], Loss: 0.0294\n",
      "Epoch [2/2], Step [140/600], Loss: 0.1289\n",
      "Epoch [2/2], Step [150/600], Loss: 0.1856\n",
      "Epoch [2/2], Step [160/600], Loss: 0.1014\n",
      "Epoch [2/2], Step [170/600], Loss: 0.1304\n",
      "Epoch [2/2], Step [180/600], Loss: 0.1291\n",
      "Epoch [2/2], Step [190/600], Loss: 0.0792\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0732\n",
      "Epoch [2/2], Step [210/600], Loss: 0.0373\n",
      "Epoch [2/2], Step [220/600], Loss: 0.0925\n",
      "Epoch [2/2], Step [230/600], Loss: 0.0779\n",
      "Epoch [2/2], Step [240/600], Loss: 0.0303\n",
      "Epoch [2/2], Step [250/600], Loss: 0.0997\n",
      "Epoch [2/2], Step [260/600], Loss: 0.0523\n",
      "Epoch [2/2], Step [270/600], Loss: 0.0287\n",
      "Epoch [2/2], Step [280/600], Loss: 0.0498\n",
      "Epoch [2/2], Step [290/600], Loss: 0.0348\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0679\n",
      "Epoch [2/2], Step [310/600], Loss: 0.0326\n",
      "Epoch [2/2], Step [320/600], Loss: 0.0382\n",
      "Epoch [2/2], Step [330/600], Loss: 0.0168\n",
      "Epoch [2/2], Step [340/600], Loss: 0.0708\n",
      "Epoch [2/2], Step [350/600], Loss: 0.0858\n",
      "Epoch [2/2], Step [360/600], Loss: 0.0418\n",
      "Epoch [2/2], Step [370/600], Loss: 0.0465\n",
      "Epoch [2/2], Step [380/600], Loss: 0.0333\n",
      "Epoch [2/2], Step [390/600], Loss: 0.1790\n",
      "Epoch [2/2], Step [400/600], Loss: 0.1189\n",
      "Epoch [2/2], Step [410/600], Loss: 0.0392\n",
      "Epoch [2/2], Step [420/600], Loss: 0.0266\n",
      "Epoch [2/2], Step [430/600], Loss: 0.0836\n",
      "Epoch [2/2], Step [440/600], Loss: 0.0102\n",
      "Epoch [2/2], Step [450/600], Loss: 0.0627\n",
      "Epoch [2/2], Step [460/600], Loss: 0.0776\n",
      "Epoch [2/2], Step [470/600], Loss: 0.1212\n",
      "Epoch [2/2], Step [480/600], Loss: 0.1510\n",
      "Epoch [2/2], Step [490/600], Loss: 0.0613\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0362\n",
      "Epoch [2/2], Step [510/600], Loss: 0.1268\n",
      "Epoch [2/2], Step [520/600], Loss: 0.1979\n",
      "Epoch [2/2], Step [530/600], Loss: 0.0327\n",
      "Epoch [2/2], Step [540/600], Loss: 0.0498\n",
      "Epoch [2/2], Step [550/600], Loss: 0.0977\n",
      "Epoch [2/2], Step [560/600], Loss: 0.1239\n",
      "Epoch [2/2], Step [570/600], Loss: 0.2078\n",
      "Epoch [2/2], Step [580/600], Loss: 0.0366\n",
      "Epoch [2/2], Step [590/600], Loss: 0.0252\n",
      "Epoch [2/2], Step [600/600], Loss: 0.1268\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1,seq_length,input_size))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        #Forward->loss->backprop->optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn(images)\n",
    "        \n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, seq_length, input_size))\n",
    "    outputs = rnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the Model\n",
    "# torch.save(rnn.state_dict(), 'rnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
